{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network to classify MNIST digits\n",
    "\n",
    "In this section, let us train a neural network to classify the MNIST dataset\n",
    "\n",
    "Let us load the data. (Courtesy of [Michael Nielson](http://neuralnetworksanddeeplearning.com/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 50000; Validation set: 10000; Test set: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist_loader\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1+np.exp(-z))\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "print \"Training set: {0}; Validation set: {1}; Test set: {2}\".format(len(training_data), len(validation_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize a test data row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110fb3290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [[ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "t = training_data[idx][0].reshape((28, 28))\n",
    "plt.imshow(t, cmap=\"gray\")\n",
    "plt.show()\n",
    "print \"Label: {0}\".format(training_data[idx][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the input data is a vector of length 784 and label data is a vector of length 10. \n",
    "\n",
    "For example, a label of $[0,0,0,1,0,0,0,0,0,0]$ refers to number 3 since the 3rd index contains 1.\n",
    "\n",
    "So, it makes sense to have 784 inputs in the first layer and 10 outputs in the output layer.\n",
    "\n",
    "Let us initialize the neural network and see how it performs on the sample above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.77642233e-01]\n",
      " [  6.89296353e-01]\n",
      " [  1.49992457e-01]\n",
      " [  1.27164454e-03]\n",
      " [  9.99027669e-01]\n",
      " [  3.23779784e-03]\n",
      " [  4.10799380e-03]\n",
      " [  9.25943036e-01]\n",
      " [  3.19141651e-04]\n",
      " [  1.39017869e-02]] 4\n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.randn(hidden_units,28*28)\n",
    "w2 = np.random.randn(10,hidden_units)\n",
    "b1 = np.random.randn(hidden_units,1)\n",
    "b2 = np.random.randn(10,1)\n",
    "\n",
    "x,y = training_data[idx]\n",
    "a0 = x\n",
    "e = y\n",
    "z1 = np.dot(w1, a0) + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = np.dot(w2, a1) + b2\n",
    "a2 = sigmoid(z2)\n",
    "        \n",
    "print a2, np.argmax(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the model. The code is similar to XOR.\n",
    "\n",
    "We simply use stochastic gradient descent by constructing random mini batches from the training data without replacement. We quantify error now by testing the model on a separate validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 9093 / 10000\n",
      "Epoch 2: 9248 / 10000\n",
      "Epoch 3: 9354 / 10000\n",
      "Epoch 4: 9428 / 10000\n",
      "Epoch 5: 9455 / 10000\n",
      "Epoch 6: 9462 / 10000\n",
      "Epoch 7: 9455 / 10000\n",
      "Epoch 8: 9513 / 10000\n",
      "Epoch 9: 9483 / 10000\n",
      "Epoch 10: 9497 / 10000\n",
      "Epoch 11: 9516 / 10000\n",
      "Epoch 12: 9538 / 10000\n",
      "Epoch 13: 9497 / 10000\n",
      "Epoch 14: 9528 / 10000\n",
      "Epoch 15: 9526 / 10000\n",
      "Epoch 16: 9521 / 10000\n",
      "Epoch 17: 9538 / 10000\n",
      "Epoch 18: 9525 / 10000\n",
      "Epoch 19: 9531 / 10000\n",
      "Epoch 20: 9534 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 30\n",
    "learning_rate = 3\n",
    "epochs = 15\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(training_data)\n",
    "    n = len(training_data)\n",
    "    mini_batches = [training_data[k:k+batch_size] for k in xrange(0, n, batch_size)]\n",
    "    \n",
    "    for mini_batch in mini_batches:\n",
    "        del_w1 = np.zeros(w1.shape)\n",
    "        del_w2 = np.zeros(w2.shape)\n",
    "        del_b1 = np.zeros(b1.shape)\n",
    "        del_b2 = np.zeros(b2.shape)\n",
    "        \n",
    "        for x,y in mini_batch:\n",
    "            a0 = x\n",
    "            e = y\n",
    "            z1 = np.dot(w1, a0) + b1\n",
    "            a1 = sigmoid(z1)\n",
    "            z2 = np.dot(w2, a1) + b2\n",
    "            a2 = sigmoid(z2)\n",
    "        \n",
    "            d2 = (a2 - e) * a2 * (1-a2)\n",
    "            del_b2 += d2\n",
    "            del_w2 += np.dot(d2, a1.T) \n",
    "            d1 = np.dot(w2.T, d2) * a1 * (1-a1)\n",
    "            del_b1 += d1\n",
    "            del_w1 += np.dot(d1, a0.T)\n",
    "            \n",
    "        w1 -= (learning_rate * del_w1 / len(mini_batch))\n",
    "        b1 -= (learning_rate * del_b1 / len(mini_batch))\n",
    "        w2 -= (learning_rate * del_w2 / len(mini_batch))\n",
    "        b2 -= (learning_rate * del_b2 / len(mini_batch))\n",
    "    \n",
    "    accurate = 0\n",
    "    for x,y in validation_data:\n",
    "        a0 = x\n",
    "        e0 = y\n",
    "        z1 = np.dot(w1, a0) + b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(w2, a1) + b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        result = np.argmax(a2)\n",
    "        if result == e0:\n",
    "            accurate += 1\n",
    "    \n",
    "    print \"Epoch {0}: {1} / {2}\".format(epoch+1, accurate, len(validation_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
